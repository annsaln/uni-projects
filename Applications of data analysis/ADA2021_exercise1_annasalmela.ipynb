{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 | TKO_2096 Application of Data Analysis 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested cross-validation for K-nearest neighbors <br>\n",
    "- Use Python 3 to program a nested cross-validation for the k-nearest neighbors (kNN) method so that the number of neighbours k is automatically selected from the range 1 to 10. In other words, the base learning algorithm is kNN but the actual learning algorithm, whose prediction performance will be evaluated with nested CV, is kNN with automatic CV-based model selection (see the lectures and the pseudo codes presented on them for more info on this interpretation).\n",
    "- As a kNN implementation, you can use sklearn: http://scikit-learn.org/stable/modules/neighbors.html but your own kNN implementation can also be used if you like to keep more control on what is happening in the learning process. The CV implementation should be easily modifiable, since the forthcoming exercises involve different problem-dependent CV variations.\n",
    "- Use the nested CV implementation on the iris data and report the resulting classification accuracy. Hint: you can use the nested CV example provided on sklearn documentation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html as a starting point and compare your nested CV implementation with that but do NOT use the ready made CV implementations of sklearn as the idea of the exercise is to learn to split the data on your own. The other exercises need more sophisticated data splitting which are not necessarily available in libraries.\n",
    "- Return your solution for each exercise BOTH as a Jupyter Notebook file and as a PDF-file made from it.\n",
    "- Return the report to the course page on **Monday 1st of February** at the latest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell import all libraries you need. For example: \n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results the nested cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n",
      "0.96\n",
      "0.9791666666666667\n"
     ]
    }
   ],
   "source": [
    "#In this cell run your script for nested CV and print the result.\n",
    "# \n",
    "\n",
    "def split_data(X, y, k):\n",
    "# split the data into equal sized folds (might result in some remainders that are not included in the split)\n",
    "    foldsize = int(len(X)/k)\n",
    "    X_folds = []\n",
    "    y_folds = []\n",
    "    for i in range(k):\n",
    "        X_fold = []\n",
    "        y_fold = []\n",
    "        while len(X_fold) < foldsize: # add rows to the fold until the fold size is met\n",
    "            X_fold.append(X[0])\n",
    "            y_fold.append(y[0])\n",
    "            X = X[1:] # since we'll shuffle the data, just remove the first row from the array\n",
    "            y = y[1:]\n",
    "        X_folds.append(X_fold)\n",
    "        y_folds.append(y_fold)\n",
    "    return X_folds, y_folds # returns k folds\n",
    "\n",
    "def find_k(X_folds, y_folds): # this is the inner loop function to find the best k\n",
    "    k_acc = []\n",
    "    for i in range(1,11): # k is an integer, 1 <= k <= 10\n",
    "        acc = []\n",
    "        for j in range(len(X_folds)-1): # do a cross validation for this k\n",
    "            X_dev = X_folds[j] # choose the jth fold as a validation set\n",
    "            X_train = np.delete(X_folds, j, 0) # delete the jth fold from the rest -> training set\n",
    "            X_train = np.concatenate([x for x in X_train]) # concatenate training folds into 2D array\n",
    "            y_dev = y_folds[j] # same for y\n",
    "            y_train = np.delete(y_folds, j, 0)\n",
    "            y_train = np.concatenate([y for y in y_train])\n",
    "            knn = KNeighborsClassifier(n_neighbors=i) # KNN with k = i\n",
    "            knn.fit(X_train, y_train)\n",
    "            pred = knn.predict(X_dev)\n",
    "            acc.append(accuracy_score(y_dev, pred)) # add the accuracy score with this split to a list\n",
    "        k_acc.append(np.average(acc)) # add the average of the accuracies with this k into a list\n",
    "    best_k = k_acc.index(max(k_acc))+1\n",
    "    return best_k\n",
    "\n",
    "def evaluate(folds): # the outer loop function, takes the number of folds as an argument\n",
    "    acc = []\n",
    "    X_folds, y_folds = split_data(X,y,folds) # splitting the data into folds\n",
    "    for i in range(folds): # outer loop for evaluation\n",
    "        X_test = X_folds[i] # take the ith fold as a test fold and delete it from the rest -> train\n",
    "        X_train = np.delete(X_folds, i, 0)\n",
    "        y_test = y_folds[i]\n",
    "        y_train = np.delete(y_folds, i, 0)\n",
    "        k = find_k(X_train, y_train) # find the best k for this train set\n",
    "        X_train = np.concatenate([x for x in X_train]) # concatenate into 2D array to evaluate\n",
    "        y_train = np.concatenate([y for y in y_train])\n",
    "        knn = KNeighborsClassifier(n_neighbors=k) # KNN with the best k\n",
    "        knn.fit(X_train, y_train)\n",
    "        pred = knn.predict(X_test)\n",
    "        acc.append(accuracy_score(y_test, pred)) # add accuracy score with the best k for this split\n",
    "    print(np.average(acc)) # prints out the accuracy average for the whole data\n",
    "\n",
    "#load the data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "evaluate(5) # give the function an integer as an argument\n",
    "evaluate(10) # data size is 150, so these result in an even split\n",
    "evaluate(8) # this works as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
